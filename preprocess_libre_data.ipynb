{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a18bec52",
   "metadata": {},
   "source": [
    "# License Notice\n",
    "\n",
    "Copyright (c) 2024 Warren Bebbington\n",
    "\n",
    "This notebook is part of the simple-glucose-analysis project and is licensed under the MIT License. For the full license text, please see the LICENSE file in the project's root directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949118f",
   "metadata": {},
   "source": [
    "# How to Backup SQLite Database from XDrip+ Android App\n",
    "\n",
    "To manually back up the SQLite database in the XDrip+ app and save it for use in your `simple_glucose_analysis` project, follow these steps:\n",
    "\n",
    "## Steps to Backup the Database\n",
    "```\n",
    "1. **Open XDrip+ App**:\n",
    "   - Launch the XDrip+ app on your Android device.\n",
    "\n",
    "2. **Access the Menu**:\n",
    "   - Tap the **hamburger menu** (three horizontal lines) located at the top right of the screen.\n",
    "\n",
    "3. **Select Import/Export**:\n",
    "   - From the dropdown menu, select **Import/Export**.\n",
    "\n",
    "4. **Export Database**:\n",
    "   - Choose the **Export Database** option.\n",
    "   - Follow any prompts to confirm the backup location if necessary.\n",
    "\n",
    "5. **Save the Database File**:\n",
    "   - When prompted to select a save location, choose a folder that is easily accessible.\n",
    "   - **Important**: Save the database file (typically named `export.sqlite`) in the main directory of your `simple_glucose_analysis` project.\n",
    "\n",
    "6. **Verify Backup**:\n",
    "   - Ensure the database file is saved correctly in your project directory. You can check this using a file explorer on your device or your computer.\n",
    "```\n",
    "## Using the Database in Your Project\n",
    "\n",
    "Once the database file is saved in the `simple_glucose_analysis` project directory, you can load it into the preprocessing notebook.\n",
    "\n",
    "**Note**: It's good practice to back up your database regularly to prevent data loss!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, inspect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9eea24",
   "metadata": {},
   "source": [
    "### Load your Xdrip+ Sqlite backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512b1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your SQLite file\n",
    "db_path = 'path-to-your-file.sqlite'\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(f'sqlite:///{db_path}')\n",
    "\n",
    "# Use SQLAlchemy's inspector to list all tables\n",
    "inspector = inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e562c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BgReadings table into a pandas DataFrame\n",
    "glucose_data = 'BgReadings'  # Table containing all BG Readings from XDrip+\n",
    "bg_df = pd.read_sql_table(glucose_data, con=engine)\n",
    "bg_df['timestamp'] = pd.to_datetime(bg_df['timestamp'], unit='ms')\n",
    "\n",
    "# Load Treatments table into a pandas DataFrame\n",
    "treatments_data = 'Treatments'  # Table containing all Treatments from XDrip+\n",
    "treatments_df = pd.read_sql_table(treatments_data, con=engine)\n",
    "treatments_df['timestamp'] = pd.to_datetime(treatments_df['timestamp'], unit='ms')\n",
    "\n",
    "# Explore the first few rows of the blood glucose table\n",
    "bg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ebb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5393a",
   "metadata": {},
   "source": [
    "We can see that the insulin column in XDrip+ is used for storing both basal and bolus insulin doses and these can be differentiated by the insulinJSON column which will show the type of insulin you set in XDrip+. In this case Novorapid(bolus) and Levemir(basal). We will create a function that loops the database and for each row in `insulin` that has any value above 0.0, we will check the insulinJSON for the word 'Novorapid' if this word is present we will move the vale to a column named `bolus` and if not we will set the value in a column named `basal`. We will then drop the rest of the rows in the treatments table.\n",
    "\n",
    "**UPDATE** - It seems the word Novorapid is not always present in the insulinJSON column and for this reason we will use the word 'Levemir' instead to try and isolate basal doses, this may be different depending on how you setup XDrip+.\n",
    "\n",
    "**UPDATE** - Neither value is consistent enough to distinguish the insulin type, for this reason i will use a cut off value of 10 units to decide if the insulin is basal or bolus. I have chosen 10 because my basal dose has always been above this and my maximum bolus dose is 6 units. This should adequatley determine which is which for my data. You may need to adjust these values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76913337",
   "metadata": {},
   "source": [
    "### Save Raw Data\n",
    "\n",
    "We will save the data in csv files for your own use. The BgReadings tables contains more data to be looked into, and there seem to be other useful tables including HeartRate(recorded by XDrip+ if health data is available on android device, eg. SmartWatch), Calibrations(calibration data), BloodReadings(Finger Prick results) and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_df.to_csv('data/raw_bg.csv')\n",
    "treatments_df.to_csv('data/raw_treaments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f41da0",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d2576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed79beb",
   "metadata": {},
   "source": [
    "### Split Basal and Bolus insulin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9364bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new columns 'bolus' and 'basal', initializing with NaN values\n",
    "treatments_df['bolus'] = float('nan')\n",
    "treatments_df['basal'] = float('nan')\n",
    "\n",
    "# Filter rows where insulin > 0\n",
    "insulin_positive = treatments_df['insulin'] > 0\n",
    "\n",
    "# Filter rows where insulin >= 10\n",
    "above_10 = treatments_df['insulin'] >= 10\n",
    "\n",
    "# For rows where 'insulin' is >= 10, assign to 'basal'\n",
    "treatments_df.loc[above_10, 'basal'] = treatments_df['insulin']\n",
    "\n",
    "# For rows where 'insulin' > 0 and 'insulin' is < 10, assign to 'bolus'\n",
    "treatments_df.loc[insulin_positive & ~above_10, 'bolus'] = treatments_df['insulin']\n",
    "\n",
    "# Display the updated DataFrame to check the result\n",
    "print(treatments_df[['insulin', 'bolus', 'basal']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5ae5a",
   "metadata": {},
   "source": [
    "## Unrequired data\n",
    "\n",
    "We will now drop all unrequired columns and adjust timestamps in both table to 5 minute intervals, improving alignment of the two data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes with only our required columns and rename calculated_value to glucose\n",
    "bg_df = bg_df[['calculated_value', 'timestamp']].copy()\n",
    "bg_df['timestamp'] = pd.to_datetime(bg_df['timestamp']).dt.round('5min')\n",
    "bg_df = bg_df.groupby('timestamp').agg({'calculated_value': 'mean'})\n",
    "bg_df.rename(columns={'calculated_value': 'glucose'}, inplace=True)\n",
    "\n",
    "# For treatments_df\n",
    "treatments_df = treatments_df[['carbs', 'basal', 'bolus', 'timestamp']].copy()\n",
    "treatments_df['timestamp'] = pd.to_datetime(treatments_df['timestamp']).dt.round('5min')\n",
    "treatments_df = treatments_df.groupby('timestamp').agg({\n",
    "    'carbs': 'sum',\n",
    "    'basal': 'sum',\n",
    "    'bolus': 'sum'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3956b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e97cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d88068",
   "metadata": {},
   "source": [
    "## Unrealistic data\n",
    "\n",
    "We will now limit glucose values to their physiological limits to help negate sensor errors, we will limit glucose levels on the upper range to no more than 20.0 mmol/l and on the lower side we will limit all glucose levels to the Libre 2 cut off limit of 2.2 mmol/l. we will also change any basal or bolus doses over 15u to 0, as these must be Xdrip+ issues, as I have never taken such large doses in my 18 years as a Type 1 Diabetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For treatments_df: Set any value above 15 in 'bolus' and 'basal' columns to 0\n",
    "treatments_df['bolus'] = treatments_df['bolus'].apply(lambda x: 0 if x > 15 else x)\n",
    "treatments_df['basal'] = treatments_df['basal'].apply(lambda x: 0 if x > 15 else x)\n",
    "\n",
    "# For bg_df: Limit glucose values to the range [2.2, 20.0] - We first need to convert mg/dl to mmol/l\n",
    "# Uncomment the below lines to adjust your data\n",
    "\n",
    "# Convert glucose from mg/dL to mmol/L using standard /18\n",
    "bg_df['glucose'] = bg_df['glucose'] / 18.0\n",
    "bg_df['glucose'] = bg_df['glucose'].clip(lower=2.2, upper=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd15aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf607cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace allbasal and bolus NaN values with 0 in the entire DataFrame\n",
    "treatments_df.fillna(0, inplace=True)\n",
    "\n",
    "treatments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b56bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa06643",
   "metadata": {},
   "source": [
    "## Combine\n",
    "\n",
    "We will now ensure the combined_df contains the complete date range and combine the two dataframes. We can then turn our focus to ensuring the glucose data is consistent and interpolate where suitable and drop rows with large gaps in the blood glucose readings data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e724dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both dataframes have a complete range of 5-minute intervals\n",
    "start_time = min(bg_df.index.min(), treatments_df.index.min())\n",
    "end_time = max(bg_df.index.max(), treatments_df.index.max())\n",
    "full_range = pd.date_range(start=start_time, end=end_time, freq='5min')\n",
    "\n",
    "bg_df = bg_df.reindex(full_range)\n",
    "treatments_df = treatments_df.reindex(full_range)\n",
    "\n",
    "# Merge the dataframes\n",
    "combined_df = pd.merge(bg_df, treatments_df, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# Explicitly name the index\n",
    "combined_df.index.name = 'timestamp'\n",
    "\n",
    "# Handle missing values\n",
    "combined_df[['carbs', 'basal', 'bolus']] = combined_df[['carbs', 'basal', 'bolus']].fillna(0) # Fill all NaN values with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f528e",
   "metadata": {},
   "source": [
    "## Check glucose data consistency\n",
    "\n",
    "We will now investigate any gaps in the `glucose` column and drop any rows with gaps over 20 minutes and drop all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867926b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify gaps in glucose readings\n",
    "combined_df['is_gap'] = combined_df['glucose'].isna()\n",
    "combined_df['gap_group'] = (combined_df['is_gap'] != combined_df['is_gap'].shift()).cumsum()\n",
    "gaps = combined_df[combined_df['is_gap']].groupby('gap_group')\n",
    "gaps_greater_than_20min = gaps.filter(lambda x: len(x) > 4)\n",
    "\n",
    "number_of_gaps = len(gaps_greater_than_20min['gap_group'].unique())\n",
    "print(f\"Number of gaps greater than 20 minutes: {number_of_gaps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a53311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv if you wish to inspect for further insight into missing glucose readings in your data\n",
    "gaps_greater_than_20min.to_csv('data/gaps_over_20min.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f25aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if number_of_gaps > 0:\n",
    "    print(\"Gaps greater than 20 minutes:\")\n",
    "    print(gaps_greater_than_20min.groupby('gap_group').sum()) # Show each group and size of group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d23111",
   "metadata": {},
   "source": [
    "## Drop rows with large gaps\n",
    "\n",
    "We will now drop all of these groups ensuring no gaps over 20 minutes will be interpolated and helping maintain the integrity of our data. We will then interpolate the remaining gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows in gaps_greater_than_20min from bg_df and interpolate the rest\n",
    "combined_df_cleaned = combined_df.drop(gaps_greater_than_20min.index)\n",
    "combined_df_cleaned['glucose'] = combined_df_cleaned['glucose'].interpolate(method='time', limit=4) # Interpolate gaps upto 20 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f57cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652cf1c",
   "metadata": {},
   "source": [
    "# Annonymise my data\n",
    "\n",
    "I will now create a `day_of_week` column and a `time` column, before dropping the timestamp column to ensure the sample data provided with this project remains somewhat annonymis. You can skip this section if your using this tool with your own data, some of the analysis functions will need altering in order to work with the actual timestamps. I will try and incorporate a settings variable in the analysis note book in order to define wether your running it with sample data, or your own data containing timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585daea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'day' column by extracting the date part of the DatetimeIndex\n",
    "combined_df_cleaned['day_of_week'] = combined_df_cleaned.index.day_name()\n",
    "\n",
    "# Create a 'time' column by extracting the time part of the DatetimeIndex\n",
    "combined_df_cleaned['time'] = combined_df_cleaned.index.time\n",
    "\n",
    "# Comment this line out to maintain the Timestamp in your data\n",
    "combined_df_cleaned = combined_df_cleaned.reset_index()\n",
    "combined_df_cleaned = combined_df_cleaned.drop('timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecebff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b99e2",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "With this notebook we have succesfully transforme the Xdrip+ data back-up from raw unaligned data into a combined dataset ready for analysis. We have done the following:\n",
    "- Loaded in the BgReading and Treatments tables from Sqlite using Pandas\n",
    "- Removed all unnecessary columns from the BgReadings and Treatments tables\n",
    "- Split the insulin column into Basal and Bolus dose columns using personal experience(adapt this section if needed)\n",
    "- Cleaned the data, including: \n",
    "    1. Renaming columns to more intuitive names\n",
    "    2. Rounding irregular time intervals into consistent 5-minute intervals and aggregating data appropriatley\n",
    "    3. Clipping and modifying unrealistic data points using common sense and personal experience(adapt this section if needed)\n",
    "    4. Addressing NaN values using a strict standard of not interpolating and gaps over 20 minutes, ensuring data validity\n",
    "    5. Complex merging of two dataframes and proper handling of missing values\n",
    "    6. Thourough consistency check in glucose data\n",
    "    7. Adaption of data to maintain temporal relationships of data whilst removing sensitive data\n",
    "    \n",
    "## Export your data\n",
    "\n",
    "If you are running the analysis on your own data you can export to a csv file now and begin the analysis. Be aware this data will span however long your backup from XDrip+ covers not just 90 days like the sample data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('data/sample_analysis_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f75c6",
   "metadata": {},
   "source": [
    "### End of Notebook\n",
    "Copyright (c) 2024 Warren Bebbington "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
