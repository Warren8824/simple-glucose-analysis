{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a18bec52",
   "metadata": {},
   "source": [
    "# License Notice\n",
    "\n",
    "Copyright (c) 2024 Warren Bebbington\n",
    "\n",
    "This notebook is part of the simple-glucose-analysis project and is licensed under the MIT License. For the full license text, please see the LICENSE file in the project's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfdab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, inspect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58654891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your SQLite file\n",
    "db_path = 'export.sqlite'\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(f'sqlite:///{db_path}')\n",
    "\n",
    "# Use SQLAlchemy's inspector to list all tables\n",
    "inspector = inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BgReadings table into a pandas DataFrame\n",
    "glucose_data = 'BgReadings'  # Table containing all BG Readings from XDrip+\n",
    "bg_df = pd.read_sql_table(glucose_data, con=engine)\n",
    "bg_df['timestamp'] = pd.to_datetime(bg_df['timestamp'], unit='ms')\n",
    "\n",
    "# Load Treatments table into a pandas DataFrame\n",
    "treatments_data = 'Treatments'  # Table containing all Treatments from XDrip+\n",
    "treatments_df = pd.read_sql_table(treatments_data, con=engine)\n",
    "treatments_df['timestamp'] = pd.to_datetime(treatments_df['timestamp'], unit='ms')\n",
    "\n",
    "# Explore the first few rows of the blood glucose table\n",
    "bg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef106138",
   "metadata": {},
   "source": [
    "We can see that the insulin column in XDrip+ is used for storing both basal and bolus insulin doses and these can be differentiated by the insulinJSON column which will show the type of insulin you set in XDrip+. In this case Novorapid(bolus) and Levemir(basal). We will create a function that loops the database and for each row in `insulin` that has any value above 0.0, we will check the insulinJSON for the word 'Novorapid' if this word is present we will move the vale to a column named `bolus` and if not we will set the value in a column named `basal`. We will then drop the rest of the rows in the treatments table.\n",
    "\n",
    "**UPDATE** - It seems the word Novorapid is not always present in the insulinJSON column and for this reason we will use the word 'Levemir' instead to try and isolate basal doses, this may be different depending on how you setup XDrip+."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d5735",
   "metadata": {},
   "source": [
    "### Save Raw Data\n",
    "\n",
    "We will save the data in csv files for your own use. The BgReadings tables contains more data to be looked into, and there seem to be other useful tables including HeartRate(recorded by XDrip+ if health data is available on android device, eg. SmartWatch), Calibrations(calibration data), BloodReadings(Finger Prick results) and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df51a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_df.to_csv('data/raw_bg.csv')\n",
    "treatments_df.to_csv('data/raw_treaments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new columns 'bolus' and 'basal', initializing with NaN values\n",
    "treatments_df['bolus'] = float('nan')\n",
    "treatments_df['basal'] = float('nan')\n",
    "\n",
    "# Filter rows where insulin > 0\n",
    "insulin_positive = treatments_df['insulin'] > 0\n",
    "\n",
    "# For rows where 'insulin' > 0 and 'insulinJSON' contains \"Levemir\", assign to 'basal'\n",
    "treatments_df.loc[insulin_positive & treatments_df['insulinJSON'].str.contains(\"Levemir\", na=False), 'basal'] = treatments_df['insulin']\n",
    "\n",
    "# For rows where 'insulin' > 0 and 'insulinJSON' does NOT contain \"Levemir\", assign to 'bolus'\n",
    "treatments_df.loc[insulin_positive & ~treatments_df['insulinJSON'].str.contains(\"Levemir\", na=False), 'bolus'] = treatments_df['insulin']\n",
    "\n",
    "# Display the updated DataFrame to check the result\n",
    "print(treatments_df[['insulin', 'insulinJSON', 'bolus', 'basal']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a3ba8",
   "metadata": {},
   "source": [
    "### Unrequired data\n",
    "\n",
    "We will now drop all unrequired columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes with only our required columns and rename calculated_value to glucose\n",
    "bg_df = bg_df[['calculated_value', 'timestamp']].copy()\n",
    "bg_df.set_index('timestamp', inplace=True)\n",
    "bg_df.rename(columns={'calculated_value': 'glucose'}, inplace=True)\n",
    "\n",
    "treatments_df = treatments_df[['carbs', 'basal', 'bolus', 'timestamp']].copy()\n",
    "treatments_df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd259f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035cda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4f6e4",
   "metadata": {},
   "source": [
    "### Resample data\n",
    "\n",
    "We will resample both tables to 5 minute intervals and sum any 5 minute periods with multiple treatments to the next 5 minutes, this will enable proper alignment of both tables whilst still maintaing the temporal relationships of treatments and blood glucose readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e140b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample bg_data to 5-minute intervals\n",
    "bg_df = bg_df.resample('5min').mean()\n",
    "\n",
    "# Resample treatments_df to 5-minute intervals, aggregating data\n",
    "# You can choose different aggregation methods, e.g., sum, mean, first, etc.\n",
    "treatments_df = treatments_df.resample('5min').sum()  # The sum of all values for the same 5-minute intervals\n",
    "\n",
    "# Create a date range covering the entire period\n",
    "full_date_range = pd.date_range(start=min(bg_df.index.min(), treatments_df.index.min()),\n",
    "                                end=max(bg_df.index.max(), treatments_df.index.max()),\n",
    "                                freq='5min')\n",
    "\n",
    "# Reindex both dataframes to this full range (this will add missing timestamps with NaNs)\n",
    "bg_df = bg_df.reindex(full_date_range)  # Fill missing values in bg_df\n",
    "treatments_df = treatments_df.reindex(full_date_range).fillna(0)  # Fill missing values in treatments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b8ddc",
   "metadata": {},
   "source": [
    "### Handle missing glucose level data\n",
    "\n",
    "We will inspect the glucose readings data for any gaps in the glucose values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c97d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify gaps in glucose readings\n",
    "bg_df['is_gap'] = bg_df['glucose'].isna()\n",
    "bg_df['gap_group'] = (bg_df['is_gap'] != bg_df['is_gap'].shift()).cumsum()\n",
    "gaps = bg_df[bg_df['is_gap']].groupby('gap_group')\n",
    "gaps_greater_than_60min = gaps.filter(lambda x: len(x) >= 12)\n",
    "\n",
    "number_of_gaps = len(gaps_greater_than_60min['gap_group'].unique())\n",
    "print(f\"Number of gaps greater than 60 minutes: {number_of_gaps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67081d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv if you wish to inspect for further insight into missing glucose readings in your data\n",
    "gaps_greater_than_60min.to_csv('data/biggaps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa4003",
   "metadata": {},
   "outputs": [],
   "source": [
    "if number_of_gaps > 0:\n",
    "    print(\"Gaps greater than 60 minutes:\")\n",
    "    print(gaps_greater_than_60min.groupby('gap_group').first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055898c",
   "metadata": {},
   "source": [
    "## Combine\n",
    "\n",
    "We will now combine the dataframes and drop all rows with more than 60 mins missing glucose readings data and interpolate all gaps smaller than this and finally add day and time columns(This is to help anonamise my data and can be skipped if you wish to use your own data and see the actual date and time). Some of the analysis script will need modifying in order to display your actual date ranges. We will then export the data to be used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes\n",
    "combined_df = pd.concat([bg_df, treatments_df], axis=1)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ecd5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5db804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'day' column by extracting the date part of the DatetimeIndex\n",
    "combined_df['day_of_week'] = combined_df.index.day_name()\n",
    "\n",
    "# Create a 'time' column by extracting the time part of the DatetimeIndex\n",
    "combined_df['time'] = combined_df.index.time\n",
    "\n",
    "# Use this line if you wish to maintain the timestamp in the data and use it in the analysis\n",
    "# combined_df['actual_timestamp'] = combined_df['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7501ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Count the number of rows in each 'gap_group'\n",
    "group_sizes = combined_df.groupby('gap_group').size()\n",
    "\n",
    "# Step 2: Identify the gap groups that are smaller than 12 rows\n",
    "small_gap_groups = group_sizes[group_sizes < 12].index\n",
    "\n",
    "# Step 3: Filter the DataFrame to include:\n",
    "# - Rows where 'gap_group' is in small_gap_groups\n",
    "# - OR rows where 'glucose' is not NaN\n",
    "filtered_df = combined_df[\n",
    "    (combined_df['gap_group'].isin(small_gap_groups)) | \n",
    "    (combined_df['glucose'].notna())\n",
    "]\n",
    "\n",
    "# Step 4: Interpolate the remaining gaps in glucose column and drop gaps columns\n",
    "filtered_df = filtered_df.copy() # Create copy of dataframe to avoid setting value in df slice warning\n",
    "filtered_df['glucose'] = filtered_df['glucose'].interpolate(method='linear')\n",
    "filtered_df = filtered_df.drop(columns=['is_gap', 'gap_group'])\n",
    "# Step 5: Reset the index and inspect the result\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ed616",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542fbce",
   "metadata": {},
   "source": [
    "## Personalisations\n",
    "\n",
    "Feel free to use the below lines to modify the data to use with your own metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below lines to adjust your data\n",
    "\n",
    "# Convert glucose from mg/dL to mmol/L using standard /18\n",
    "#filtered_df['glucose'] = filtered_df['glucose'] / 18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337720b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6188908",
   "metadata": {},
   "source": [
    "## Export your data\n",
    "\n",
    "If you are running the analysis on your own data you can export to a csv file now and begin the analysis. Be aware this data will span however long your backup from XDrip+ covers not just 90 days like the sample data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('data/processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b4255",
   "metadata": {},
   "source": [
    "### End of Notebook\n",
    "(c) 2024 Warren Bebbington "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
